{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/RaghunandanVenkatesh/LearningToSee/blob/master/RL_Hvac.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2YfFFC-fHwjX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import UnivariateSpline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "UkUpOfRnedCy"
   },
   "outputs": [],
   "source": [
    "#inputs for plant\n",
    "T_oat = 30.0\n",
    "PWM_front_box = 30.0\n",
    "T_enginewater_set = 80.0\n",
    "POS_fresh_air_flap = 40.0\n",
    "value = 0\n",
    "dt = 1 # sample time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qmqVJzY5siai"
   },
   "outputs": [],
   "source": [
    "# initialization \n",
    "water_val_pos_list = np.array([0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100])\n",
    "A_list = np.array([0, 0.6011, .61, 0.6, 1.88, 1.88, 2.1, 2.1, 2.1, 2.5, 3.5])\n",
    "lookup = UnivariateSpline(water_val_pos_list, A_list, k=1, s=0.0)\n",
    "T_enginewater = T_oat\n",
    "T_fap = T_oat\n",
    "hA_screen = 0.0007\n",
    "hA_shell = 0.0029\n",
    "convfactor = 0.0016\n",
    "mcp_shell = 0.83\n",
    "T_shell = T_oat\n",
    "T_set = 24\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XE2tpubTsjMF"
   },
   "source": [
    "# Engine water temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "TJHYPS64sjkm"
   },
   "outputs": [],
   "source": [
    "T_enginewater = T_oat + ( T_enginewater_set + T_oat - T_enginewater) * (1 - np.exp(-value)) \n",
    "value = np.min([value+0.002, 5]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_6rwTCte2hR"
   },
   "source": [
    "# Airoutlet temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dttxVWagH2yJ"
   },
   "outputs": [],
   "source": [
    "T_air_in = T_oat * POS_fresh_air_flap/100 + T_fap * (1 - POS_fresh_air_flap/100)\n",
    "A = lookup(POS_fresh_air_flap)\n",
    "eff = 1 - np.exp(-POS_fresh_air_flap*A/100)\n",
    "T_airout = T_air_in + (T_enginewater - T_air_in) * eff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZlZ-Mlwe62I"
   },
   "source": [
    "# room temperature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "0gy1fz8deTeU"
   },
   "outputs": [],
   "source": [
    "d_T_fap = hA_screen * (T_oat - T_fap) + hA_shell * (T_shell - T_fap)\n",
    "d_T_fap += PWM_front_box * convfactor * 0.718 * (T_airout - T_fap)\n",
    "T_fap = d_T_fap * dt + T_fap\n",
    "T_shell = T_shell - hA_shell * (T_shell - T_fap)/mcp_shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "IWyxvD0f9RZE"
   },
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, T_oat, PWM_front_box, T_enginewater_set, POS_fresh_air_flap, T_set ):\n",
    "        self.T_oat = T_oat\n",
    "        self.PWM_front_box = PWM_front_box\n",
    "        self.T_enginewater_set = T_enginewater_set\n",
    "        self.POS_fresh_air_flap = POS_fresh_air_flap\n",
    "        self.value = 0\n",
    "        self.dt = 1 # sample time\n",
    "        self.T_set = T_set\n",
    "        # initialization \n",
    "        water_val_pos_list = np.array([0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100])\n",
    "        A_list = np.array([0, 0.6011, .61, 0.6, 1.88, 1.88, 2.1, 2.1, 2.1, 2.5, 3.5])\n",
    "        self.lookup = UnivariateSpline(water_val_pos_list, A_list, k=1, s=0.0)\n",
    "        self.T_enginewater = T_oat\n",
    "        self.T_fap = T_oat\n",
    "        self.hA_screen = 0.0007\n",
    "        self.hA_shell = 0.0029\n",
    "        self.convfactor = 0.0016\n",
    "        self.mcp_shell = 0.83\n",
    "        self.T_shell = T_oat\n",
    "\n",
    "    def step(self):\n",
    "        # engine water temp\n",
    "        self.T_enginewater = self.T_oat + ( self.T_enginewater_set + self.T_oat - self.T_enginewater) * (1 - np.exp(-self.value)) \n",
    "        self.value = np.min([self.value+0.002, 5])\n",
    "        # air outlet temp\n",
    "        T_air_in = self.T_oat * self.POS_fresh_air_flap/100 + self.T_fap * (1 - self.POS_fresh_air_flap/100)\n",
    "        A = self.lookup(self.POS_fresh_air_flap)\n",
    "        eff = 1 - np.exp(-self.POS_fresh_air_flap*A/100)\n",
    "        T_airout = T_air_in + (self.T_enginewater - T_air_in) * eff \n",
    "        # room temperature\n",
    "        d_T_fap = self.hA_screen * (self.T_oat - self.T_fap) + self.hA_shell * (self.T_shell - self.T_fap)\n",
    "        d_T_fap += self.PWM_front_box * self.convfactor * 0.718 * (T_airout - self.T_fap)\n",
    "        self.T_fap = d_T_fap * dt + self.T_fap\n",
    "        self.T_shell = self.T_shell - self.hA_shell * (self.T_shell - self.T_fap)/self.mcp_shell    \n",
    "\n",
    "        #reward\n",
    "        reward = -(self.T_set - self.T_fap)**2\n",
    "        observation = [self.T_fap]\n",
    "        info = False\n",
    "        return observation, reward, info\n",
    "\n",
    "    def reset(self):\n",
    "        self.T_shell = T_oat\n",
    "        self.T_fap = T_oat\n",
    "        self.T_enginewater = T_oat\n",
    "        observation = [self.T_fap]\n",
    "        return observation                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1gfDie52LfwN"
   },
   "outputs": [],
   "source": [
    "upper_temp_fap = 60\n",
    "min_temp_fap = -40\n",
    "env = Environment( T_oat, PWM_front_box, T_enginewater_set, POS_fresh_air_flap, T_set )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wo8QIUFNiKy5",
    "outputId": "d3eb8b3f-438f-4633-d1cd-e0c201c21a14"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([30.0], -36.0, False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "VoR3IV9qiY2a"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import gym\n",
    "from gym import spaces, logger\n",
    "from gym.utils import seeding\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "xlbAVjS_jHFt"
   },
   "outputs": [],
   "source": [
    "class HvacPlantEnv(gym.Env):\n",
    "    def __init__(self, T_oat, T_enginewater_set, T_set):\n",
    "        self.dt = 1 # sample time\n",
    "        # initialization \n",
    "        water_val_pos_list = np.array([0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100])\n",
    "        A_list = np.array([0, 0.6011, .61, 0.6, 1.88, 1.88, 2.1, 2.1, 2.1, 2.5, 3.5])\n",
    "        self.lookup = UnivariateSpline(water_val_pos_list, A_list, k=1, s=0.0)\n",
    "        self.hA_screen = 0.0007\n",
    "        self.hA_shell = 0.0029\n",
    "        self.convfactor = 0.0016\n",
    "        self.mcp_shell = 0.83\n",
    "        self.T_oat = T_oat\n",
    "        self.T_enginewater_set = T_enginewater_set\n",
    "        self.T_set = T_set\n",
    "        self.action_space = spaces.Tuple((\n",
    "                                spaces.Discrete(100),\n",
    "                                spaces.Discrete(100)))\n",
    "        self.observation_space = spaces.Box(-40, 80, shape=(4,) ,dtype=np.float32)\n",
    "\n",
    "\n",
    "        self.state = None\n",
    "\n",
    "        self.steps_beyond_done = None\n",
    "\n",
    "    def step(self, action):\n",
    "        T_shell, T_fap, T_enginewater, value = self.state\n",
    "        POS_fresh_air_flap = action[0]\n",
    "        PWM_front_box = action[1] \n",
    "        # engine water temp\n",
    "        T_enginewater = self.T_oat + ( self.T_enginewater_set + self.T_oat - T_enginewater) * (1 - np.exp(-value)) \n",
    "        value = np.min([value+0.002, 5])\n",
    "        # air outlet temp\n",
    "        T_air_in = self.T_oat * POS_fresh_air_flap/100 + T_fap * (1 - POS_fresh_air_flap/100)\n",
    "        A = self.lookup(POS_fresh_air_flap)\n",
    "        eff = 1 - np.exp(-POS_fresh_air_flap*A/100)\n",
    "        T_airout = T_air_in + (T_enginewater - T_air_in) * eff \n",
    "        # room temperature\n",
    "        d_T_fap = self.hA_screen * (self.T_oat - T_fap) + self.hA_shell * (T_shell - T_fap)\n",
    "        d_T_fap += PWM_front_box * self.convfactor * 0.718 * (T_airout - T_fap)\n",
    "        T_fap = d_T_fap * self.dt + T_fap\n",
    "        T_shell = T_shell - self.hA_shell * (T_shell - T_fap)/self.mcp_shell \n",
    "        self.state = (T_shell, T_fap, T_enginewater, value)   \n",
    "\n",
    "        #reward\n",
    "        done  = self.T_set == T_fap\n",
    "        if not done:\n",
    "            reward = np.abs(self.T_set - T_fap)\n",
    "        elif self.steps_beyond_done is  None:\n",
    "            self.steps_beyond_done = 0\n",
    "            reward = np.abs(self.T_set - T_fap)\n",
    "        else:\n",
    "            if self.steps_beyond_done == 0:\n",
    "                print(\n",
    "                  \"You are calling 'step()' even though this \"\n",
    "                  \"environment has already returned done = True. You \"\n",
    "                  \"should always call 'reset()' once you receive 'done = \"\n",
    "                  \"True' -- any further steps are undefined behavior.\")\n",
    "            self.steps_beyond_done += 1\n",
    "            reward = 0.0\n",
    "        return np.array(self.state), reward, done, {} \n",
    "\n",
    "    def reset(self):\n",
    "        # self.T_oat = T_oat\n",
    "        # self.T_enginewater_set = T_enginewater\n",
    "        self.state = [self.T_oat, self.T_oat, self.T_oat, 0]\n",
    "        self.steps_beyond_done = None\n",
    "        return np.array(self.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dGCVSFnBu2Tz",
    "outputId": "a70fbfe7-0521-458a-f770-fe7ba3accb9b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.4e+01, 2.4e+01, 2.4e+01, 2.0e-03]), 6.0, False, {})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = spaces.Tuple((spaces.Discrete(10), spaces.Discrete(100))).sample()\n",
    "env_h = HvacPlantEnv(24,80,30)\n",
    "env_h.reset()\n",
    "env_h.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 60)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# env_h.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "IXoVnjwlI3LC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "state = env_h.reset()\n",
    "action = env_h.action_space.sample()\n",
    "obs, rew , done, _ = env_h.step(action)\n",
    "print(state.shape)\n",
    "print(env_h.observation_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action_list = []\n",
    "# for _ in range(50):\n",
    "#     action = env_h.action_space.sample()\n",
    "#     # print(action)\n",
    "#     action_list.append(action)\n",
    "# for i in range(len(action_list)):\n",
    "#     obs,rew,done,_ = env_h.step(action_list[i])\n",
    "#     print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 14292.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nb_steps =100\n",
    "cum_rew = 0\n",
    "i = 0\n",
    "for i in tqdm(range(nb_steps)): \n",
    "    action = env_h.action_space.sample()\n",
    "    x, reward, done, _ = env_h.step(action)\n",
    "    #print(reward)\n",
    "    cum_rew += rew\n",
    "    i+=1\n",
    "print(cum_rew/i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DDPG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DDPGAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.random import OrnsteinUhlenbeckProcess\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input, Concatenate\n",
    "from tensorflow.keras import initializers, regularizers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_actions = len(env_h.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "window_length = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\python 3.7\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 17)                289       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 36        \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 405\n",
      "Trainable params: 405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "actor = Sequential()\n",
    "# The network's input fits the observation space of the env\n",
    "actor.add(Flatten(input_shape=(window_length,) + (4,)))  # observation_space.shape != (4,).. ## todo: correct it\n",
    "actor.add(Dense(16, activation='relu'))\n",
    "actor.add(Dense(17, activation='relu'))\n",
    "# The network output fits the action space of the env\n",
    "actor.add(Dense(nb_actions,\n",
    "                kernel_initializer=initializers.RandomNormal(stddev=1e-5),\n",
    "                activation='sigmoid',\n",
    "                kernel_regularizer=regularizers.l2(1e-2)))\n",
    "actor.add(tf.keras.layers.Lambda(lambda x: x * 100))\n",
    "print(actor.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "observation_input (InputLayer)  [(None, 1, 4)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "action_input (InputLayer)       [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 4)            0           observation_input[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 6)            0           action_input[0][0]               \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 32)           224         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 32)           1056        dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            33          dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,313\n",
      "Trainable params: 1,313\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#todo : Normalise inputs(action and oservation)\n",
    "action_input = Input(shape=(nb_actions,), name='action_input')\n",
    "observation_input = Input(shape=(window_length,) + (4,), name='observation_input')\n",
    "# (using keras functional API)\n",
    "flattened_observation = Flatten()(observation_input)\n",
    "x = Concatenate()([action_input, flattened_observation])\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "#x = Dense(32, activation='relu')(x)\n",
    "x = Dense(1, activation='linear')(x)\n",
    "critic = Model(inputs=(action_input, observation_input), outputs=x)\n",
    "print(critic.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(\n",
    "    limit=5000,\n",
    "    window_length=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random process for exploration during training\n",
    "# this is essential for the DDPG algorithm\n",
    "random_process = OrnsteinUhlenbeckProcess(\n",
    "    theta=0.5,\n",
    "    mu=0.0,\n",
    "    sigma=0.1,\n",
    "    dt=0.001,\n",
    "    sigma_min=0.05,\n",
    "    n_steps_annealing=85000,\n",
    "    size=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DDPGAgent(\n",
    "    # Pass the previously defined characteristics\n",
    "    nb_actions=nb_actions,\n",
    "    actor=actor,\n",
    "    critic=critic,\n",
    "    critic_action_input=action_input,\n",
    "    memory=memory,\n",
    "    random_process=random_process,\n",
    "\n",
    "    # Define the overall training parameters\n",
    "    nb_steps_warmup_actor=2048,\n",
    "    nb_steps_warmup_critic=1024,\n",
    "    target_model_update=1000,\n",
    "    gamma=0.99,\n",
    "    batch_size=128,\n",
    "    memory_interval=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.compile([Adam(lr=3e-5), Adam(lr=3e-3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n",
      "  500/10000: episode: 1, duration: 0.344s, episode steps: 500, steps per second: 1454, episode reward: 4470.219, mean reward:  8.940 [ 0.011, 16.211], mean action: 50.012 [49.918, 50.119],  loss: --, mean_q: --\n",
      " 1000/10000: episode: 2, duration: 0.282s, episode steps: 500, steps per second: 1773, episode reward: 4467.662, mean reward:  8.935 [ 0.004, 16.207], mean action: 49.895 [49.752, 50.001],  loss: --, mean_q: --\n",
      " 1500/10000: episode: 3, duration: 2.463s, episode steps: 500, steps per second: 203, episode reward: 4479.938, mean reward:  8.960 [ 0.011, 16.250], mean action: 50.110 [50.045, 50.212],  loss: 1.726865, mean_q: -0.010476\n",
      " 2000/10000: episode: 4, duration: 2.309s, episode steps: 500, steps per second: 217, episode reward: 4480.573, mean reward:  8.961 [ 0.022, 16.250], mean action: 50.148 [50.075, 50.217],  loss: 0.721814, mean_q: 0.100356\n",
      " 2500/10000: episode: 5, duration: 2.569s, episode steps: 500, steps per second: 195, episode reward: 2979.204, mean reward:  5.958 [ 0.027, 15.481], mean action: 39.766 [26.721, 49.990],  loss: 5.127648, mean_q: 19.351179\n",
      " 3000/10000: episode: 6, duration: 2.528s, episode steps: 500, steps per second: 198, episode reward: 4190.699, mean reward:  8.381 [ 0.019, 14.243], mean action: 43.209 [6.333, 68.355],  loss: 3.413850, mean_q: 18.931723\n",
      " 3500/10000: episode: 7, duration: 2.538s, episode steps: 500, steps per second: 197, episode reward: 1275.021, mean reward:  2.550 [ 0.006,  6.000], mean action: 31.285 [2.567, 72.504],  loss: 10.421428, mean_q: 40.450836\n",
      " 4000/10000: episode: 8, duration: 2.601s, episode steps: 500, steps per second: 192, episode reward: 4014.141, mean reward:  8.028 [ 0.008, 15.329], mean action: 38.509 [17.627, 53.743],  loss: 5.780338, mean_q: 39.294670\n",
      " 4500/10000: episode: 9, duration: 2.629s, episode steps: 500, steps per second: 190, episode reward: 4212.904, mean reward:  8.426 [ 0.032, 15.388], mean action: 42.696 [32.945, 48.181],  loss: 8.185098, mean_q: 52.244659\n",
      " 5000/10000: episode: 10, duration: 2.621s, episode steps: 500, steps per second: 191, episode reward: 4160.638, mean reward:  8.321 [ 0.017, 15.304], mean action: 41.544 [35.230, 48.153],  loss: 6.202617, mean_q: 52.412403\n",
      " 5500/10000: episode: 11, duration: 2.576s, episode steps: 500, steps per second: 194, episode reward: 4216.815, mean reward:  8.434 [ 0.012, 15.347], mean action: 42.714 [37.536, 48.958],  loss: 8.923945, mean_q: 65.924309\n",
      " 6000/10000: episode: 12, duration: 2.589s, episode steps: 500, steps per second: 193, episode reward: 4221.721, mean reward:  8.443 [ 0.026, 15.395], mean action: 42.668 [38.164, 47.285],  loss: 7.830844, mean_q: 66.232857\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "agent.fit(\n",
    "    env_h,\n",
    "    nb_steps=10000,\n",
    "    nb_max_start_steps=0,\n",
    "    nb_max_episode_steps=500,\n",
    "    visualize=False,\n",
    "    action_repetition=1,\n",
    "    verbose=2,\n",
    "    log_interval=50,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent\n",
    "hist = agent.test(\n",
    "    env_h,\n",
    "    nb_episodes=1,\n",
    "    action_repetition=5,\n",
    "    visualize=False,\n",
    "    nb_max_episode_steps=2000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPT4wX6IYkga0MCEH6Z3z09",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "RL_Hvac.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
