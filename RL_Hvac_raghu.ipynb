{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "RL_Hvac_raghu.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RaghunandanVenkatesh/LearningToSee/blob/master/RL_Hvac_raghu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YfFFC-fHwjX",
        "outputId": "7154ff3e-7fa8-432d-a69e-c8e76c91d7ea"
      },
      "source": [
        "!pip install keras-rl2\n",
        "!pip install --upgrade tensorflow==2.2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras-rl2 in /usr/local/lib/python3.7/dist-packages (1.0.4)\n",
            "Requirement already satisfied: tensorflow>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from keras-rl2) (2.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.12.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (0.36.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.1.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (0.10.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (3.12.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (2.2.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.19.5)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (2.10.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (0.3.3)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (2.2.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.32.0)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorflow>=2.1.0->keras-rl2) (54.0.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1.0->keras-rl2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1.0->keras-rl2) (3.3.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1.0->keras-rl2) (0.4.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1.0->keras-rl2) (1.8.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1.0->keras-rl2) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1.0->keras-rl2) (1.27.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1.0->keras-rl2) (3.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1.0->keras-rl2) (1.3.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1.0->keras-rl2) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1.0->keras-rl2) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1.0->keras-rl2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1.0->keras-rl2) (2.10)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1.0->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1.0->keras-rl2) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1.0->keras-rl2) (4.2.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1.0->keras-rl2) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1.0->keras-rl2) (3.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1.0->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1.0->keras-rl2) (0.4.8)\n",
            "Requirement already up-to-date: tensorflow==2.2 in /usr/local/lib/python3.7/dist-packages (2.2.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (0.36.2)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (2.2.2)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (1.32.0)\n",
            "Requirement already satisfied, skipping upgrade: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (1.6.3)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (2.2.0)\n",
            "Requirement already satisfied, skipping upgrade: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (0.4.3)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (54.0.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.8.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.3.4)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.27.1)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.7.2)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (4.2.1)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (4.7.2)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kl6EXqpINmPA"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import UnivariateSpline\n",
        "\n",
        "import math\n",
        "import gym\n",
        "from gym import spaces, logger\n",
        "from gym.utils import seeding\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from rl.agents import DDPGAgent\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.random import OrnsteinUhlenbeckProcess\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Input, Concatenate\n",
        "from tensorflow.keras import initializers, regularizers\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9x-wGcvU1_E"
      },
      "source": [
        ""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkUpOfRnedCy"
      },
      "source": [
        "\n",
        "\n",
        "#inputs for plant\n",
        "T_oat = 12\n",
        "\n",
        "T_enginewater_set = 80.0\n",
        "T_set = 24"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlbAVjS_jHFt"
      },
      "source": [
        "class HvacPlantEnv(gym.Env):\n",
        "    def __init__(self, T_oat, T_enginewater_set):\n",
        "        self.dt = 1 # sample time\n",
        "        # initialization \n",
        "        water_val_pos_list = np.array([0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100])\n",
        "        A_list = np.array([0, 0.6011, .61, 0.6, 1.88, 1.88, 2.1, 2.1, 2.1, 2.5, 3.5])\n",
        "        self.lookup = UnivariateSpline(water_val_pos_list, A_list, k=1, s=0.0)\n",
        "        self.hA_screen = 0.0007\n",
        "        self.hA_shell = 0.0029\n",
        "        self.convfactor = 0.0016\n",
        "        self.mcp_shell = 0.83\n",
        "        self.T_oat = T_oat\n",
        "        self.T_enginewater_set = T_enginewater_set\n",
        "        self.T_set = 0\n",
        "        self.value = 0\n",
        "        self.counter = 0\n",
        "        self.action_space = spaces.Tuple((\n",
        "                                spaces.Discrete(100),\n",
        "                                spaces.Discrete(100)))\n",
        "        diff = np.abs(T_set - T_oat)\n",
        "        self.observation_space = spaces.Box(0, 1, shape=(3,) ,dtype=np.float32)\n",
        "\n",
        "        \n",
        "        self.state = None\n",
        "\n",
        "        self.steps_beyond_done = None\n",
        "\n",
        "    def step(self, action):\n",
        "        T_shell, T_fap, T_enginewater = self.state \n",
        "        T_shell *= self.T_set\n",
        "        T_fap *= self.T_set\n",
        "        T_enginewater *= self.T_enginewater_set\n",
        "        POS_fresh_air_flap = action[0]*100\n",
        "        PWM_front_box = action[1] *100\n",
        "        # engine water temp\n",
        "        T_enginewater = self.T_oat + ( self.T_enginewater_set + self.T_oat - T_enginewater) * (1 - np.exp(-self.value)) \n",
        "        self.value = np.min([self.value+0.002, 5])\n",
        "        # air outlet temp\n",
        "        self.counter += 1\n",
        "        T_air_in = self.T_oat * POS_fresh_air_flap/100 + T_fap * (1 - POS_fresh_air_flap/100)\n",
        "        A = self.lookup(POS_fresh_air_flap)\n",
        "        eff = 1 - np.exp(-POS_fresh_air_flap*A/100)\n",
        "        T_airout = T_air_in + (T_enginewater - T_air_in) * eff\n",
        "        #print(counter)\n",
        "        if self.counter % 600 == 0:\n",
        "          print(self.counter)\n",
        "          print(POS_fresh_air_flap, PWM_front_box) \n",
        "          print(self.T_set - T_fap)\n",
        "        # room temperature\n",
        "        d_T_fap = self.hA_screen * (self.T_oat - T_fap) + self.hA_shell * (T_shell - T_fap)\n",
        "        d_T_fap += PWM_front_box * self.convfactor * 0.718 * (T_airout - T_fap)\n",
        "        T_fap = d_T_fap * self.dt + T_fap\n",
        "        T_shell = T_shell - self.hA_shell * (T_shell - T_fap)/self.mcp_shell \n",
        "        self.state = (T_shell/self.T_set, T_fap/self.T_set, T_enginewater/self.T_enginewater_set)   \n",
        "\n",
        "        #reward\n",
        "        done  = self.T_set == T_fap\n",
        "        if self.T_set-0.5 < T_fap < self.T_set+0.5:\n",
        "            reward = 1\n",
        "        \n",
        "            print('reward highest for count', self.counter)\n",
        "            #reward = -np.abs(self.T_set - T_fap)\n",
        "            \n",
        "            #print(reward)\n",
        "        else:\n",
        "            #reward = -(1 - T_fap/self.T_set)**2\n",
        "            reward = -np.abs(self.T_set - T_fap)\n",
        "        '''\n",
        "        elif self.steps_beyond_done is  None:\n",
        "            \n",
        "            self.steps_beyond_done = 0\n",
        "            reward = -np.abs(self.T_set - T_fap)\n",
        "\n",
        "        else:\n",
        "            if self.steps_beyond_done == 0:\n",
        "                print(\n",
        "                  \"You are calling 'step()' even though this \"\n",
        "                  \"environment has already returned done = True. You \"\n",
        "                  \"should always call 'reset()' once you receive 'done = \"\n",
        "                  \"True' -- any further steps are undefined behavior.\")\n",
        "            self.steps_beyond_done += 1\n",
        "            reward = 0.0\n",
        "        '''\n",
        "        return np.array(self.state), reward, done, {} \n",
        "\n",
        "    def reset(self):\n",
        "        # self.T_oat = T_oat\n",
        "        # self.T_enginewater_set = T_enginewater\n",
        "\n",
        "        self.value = 0\n",
        "        self.counter = 0\n",
        "        self.T_set = np.random.randint(self.T_oat, 40,1)[0]\n",
        "        self.state = [self.T_oat/self.T_set, self.T_oat/self.T_set, self.T_oat/self.T_enginewater_set]\n",
        "        self.steps_beyond_done = None\n",
        "        return np.array(self.state)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGCVSFnBu2Tz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8dffe32-2dba-4c1a-c419-9f57982e2b72"
      },
      "source": [
        "action = spaces.Tuple((spaces.Discrete(10), spaces.Discrete(100))).sample()\n",
        "env_h = HvacPlantEnv(T_oat, T_enginewater_set)\n",
        "env_h.reset()\n",
        "env_h.step(action)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0.34285714, 0.34285714, 0.15      ]), -23.0, False, {})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdOL2AG9Kkw3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae6a3e95-332f-4e56-b465-79ac0cc5807a"
      },
      "source": [
        "# env_h.action_space.sample()\n",
        "np.random.randint(-15,24,1)[0]"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXoVnjwlI3LC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c75639ca-6851-45bb-b5e6-58d4e880c827"
      },
      "source": [
        "state = env_h.reset()\n",
        "action = env_h.action_space.sample()\n",
        "obs, rew , done, _ = env_h.step(action)\n",
        "print(state.shape)\n",
        "print(env_h.observation_space.shape)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3,)\n",
            "(3,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aG2uT4Q5Kkw8"
      },
      "source": [
        "# action_list = []\n",
        "# for _ in range(50):\n",
        "#     action = env_h.action_space.sample()\n",
        "#     # print(action)\n",
        "#     action_list.append(action)\n",
        "# for i in range(len(action_list)):\n",
        "#     obs,rew,done,_ = env_h.step(action_list[i])\n",
        "#     print(obs)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuchMdz6Kkw-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b86e126-3ea4-4ec5-ecf4-3084ac572c2b"
      },
      "source": [
        "nb_steps =100\n",
        "cum_rew = 0\n",
        "i = 0\n",
        "for i in tqdm(range(nb_steps)): \n",
        "    action = env_h.action_space.sample()\n",
        "    x, reward, done, _ = env_h.step(action)\n",
        "    #print(reward)\n",
        "    cum_rew += rew\n",
        "    i+=1\n",
        "print(cum_rew/i)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 8748.16it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-4.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pctMfvheKkxA"
      },
      "source": [
        "DDPG Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2zmRYLNKkxC"
      },
      "source": [
        "nb_actions = len(env_h.action_space.sample())"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94CVJBTGKkxD",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fc0b5a5-3608-4b41-f2a9-355cba10791d"
      },
      "source": [
        "window_length = 1\n",
        "min(10,100)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rlsD-ckKkxE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d8c0913-1d9e-4cce-f99f-f77dfd5b900f"
      },
      "source": [
        "actor = Sequential()\n",
        "# The network's input fits the observation space of the env\n",
        "actor.add(Flatten(input_shape=(window_length,) + (3,)))  # observation_space.shape != (4,).. ## todo: correct it\n",
        "actor.add(Dense(16, activation='relu'))\n",
        "actor.add(Dense(17, activation='relu'))\n",
        "# The network output fits the action space of the env\n",
        "actor.add(Dense(nb_actions,\n",
        "                kernel_initializer=initializers.RandomNormal(stddev=1e-5),\n",
        "                activation='sigmoid',\n",
        "                kernel_regularizer=regularizers.l2(1e-2)))\n",
        "#actor.add(tf.keras.layers.Lambda(lambda x: x*100))\n",
        "print(actor.summary())"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_4 (Flatten)          (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 17)                289       \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 2)                 36        \n",
            "=================================================================\n",
            "Total params: 389\n",
            "Trainable params: 389\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpSJHU7eKkxH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed7b4878-e26b-4abc-f36a-0d324e9ba3a3"
      },
      "source": [
        "#todo : Normalise inputs(action and oservation)\n",
        "action_input = Input(shape=(nb_actions,), name='action_input')\n",
        "observation_input = Input(shape=(window_length,) + (3,), name='observation_input')\n",
        "# (using keras functional API)\n",
        "flattened_observation = Flatten()(observation_input)\n",
        "x = Concatenate()([action_input, flattened_observation])\n",
        "x = Dense(32, activation='relu')(x)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "#x = Dense(32, activation='relu')(x)\n",
        "x = Dense(1, activation='linear')(x)\n",
        "critic = Model(inputs=(action_input, observation_input), outputs=x)\n",
        "print(critic.summary())"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "observation_input (InputLayer)  [(None, 1, 3)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "action_input (InputLayer)       [(None, 2)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_5 (Flatten)             (None, 3)            0           observation_input[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 5)            0           action_input[0][0]               \n",
            "                                                                 flatten_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_15 (Dense)                (None, 32)           192         concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_16 (Dense)                (None, 32)           1056        dense_15[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_17 (Dense)                (None, 1)            33          dense_16[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 1,281\n",
            "Trainable params: 1,281\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYFbiexhKkxJ"
      },
      "source": [
        "memory = SequentialMemory(\n",
        "    limit=4000,\n",
        "    window_length=1\n",
        ")"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1syBDsreKkxK"
      },
      "source": [
        "# Create a random process for exploration during training\n",
        "# this is essential for the DDPG algorithm\n",
        "random_process = OrnsteinUhlenbeckProcess(\n",
        "    theta=0.5,\n",
        "    mu=0.0,\n",
        "    sigma=0.1,\n",
        "    dt=0.0001,\n",
        "    sigma_min=0.01,\n",
        "    n_steps_annealing=3600*4,\n",
        "    size=2\n",
        ")"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBj_qQFdKkxL"
      },
      "source": [
        "agent = DDPGAgent(\n",
        "    # Pass the previously defined characteristics\n",
        "    nb_actions=nb_actions,\n",
        "    actor=actor,\n",
        "    critic=critic,\n",
        "    critic_action_input=action_input,\n",
        "    memory=memory,\n",
        "    random_process=random_process,\n",
        "\n",
        "    # Define the overall training parameters\n",
        "    nb_steps_warmup_actor=2048,\n",
        "    nb_steps_warmup_critic=1024,\n",
        "    target_model_update=2500,\n",
        "    gamma=0.9,\n",
        "    batch_size=128,\n",
        "    memory_interval=2\n",
        ")"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bcStwpZKkxM"
      },
      "source": [
        "\n",
        "agent.compile(optimizer = [Adam(lr=3e-6), Adam(lr=3e-5)])\n",
        "#agent.compile(Adam(lr=3e-5))"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6zOi9yrKkxM",
        "outputId": "f925aebd-207f-4125-e5e9-0434f044c872"
      },
      "source": [
        "# Training\n",
        "agent.fit(\n",
        "    env_h,\n",
        "    nb_steps=3600*24,\n",
        "    nb_max_start_steps=0,\n",
        "    nb_max_episode_steps=3600,\n",
        "    visualize=False,\n",
        "    action_repetition=1,\n",
        "    verbose=2,\n",
        "    log_interval=50,\n",
        "\n",
        ")"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 86400 steps ...\n",
            "reward highest for count 174\n",
            "reward highest for count 175\n",
            "reward highest for count 176\n",
            "reward highest for count 177\n",
            "reward highest for count 178\n",
            "reward highest for count 179\n",
            "reward highest for count 180\n",
            "reward highest for count 181\n",
            "reward highest for count 182\n",
            "reward highest for count 183\n",
            "reward highest for count 184\n",
            "reward highest for count 185\n",
            "reward highest for count 186\n",
            "reward highest for count 187\n",
            "reward highest for count 188\n",
            "reward highest for count 189\n",
            "reward highest for count 190\n",
            "reward highest for count 191\n",
            "600\n",
            "55.93292713165283 49.77615773677826\n",
            "-11.884205301851523\n",
            "1200\n",
            "52.988290786743164 50.59956908226013\n",
            "-15.824190601625595\n",
            "1800\n",
            "51.42860412597656 47.58048355579376\n",
            "-16.64627315585576\n",
            "2400\n",
            "54.76998686790466 49.01360273361206\n",
            "-17.678377998384164\n",
            "3000\n",
            "54.10219430923462 45.06487846374512\n",
            "-17.501263909407456\n",
            "3600\n",
            "58.52212905883789 42.307111620903015\n",
            "-18.409358788589415\n",
            "  3600/86400: episode: 1, duration: 22.363s, episode steps: 3600, steps per second: 161, episode reward: -54258.099, mean reward: -15.072 [-18.410,  1.000], mean action: 0.511 [0.422, 0.589],  loss: 87.013262, mean_q: -1.192149\n",
            "reward highest for count 258\n",
            "reward highest for count 259\n",
            "reward highest for count 260\n",
            "reward highest for count 261\n",
            "reward highest for count 262\n",
            "reward highest for count 263\n",
            "reward highest for count 264\n",
            "reward highest for count 265\n",
            "reward highest for count 266\n",
            "reward highest for count 267\n",
            "reward highest for count 268\n",
            "reward highest for count 269\n",
            "reward highest for count 270\n",
            "reward highest for count 271\n",
            "reward highest for count 272\n",
            "reward highest for count 273\n",
            "reward highest for count 274\n",
            "reward highest for count 275\n",
            "reward highest for count 276\n",
            "reward highest for count 277\n",
            "reward highest for count 278\n",
            "reward highest for count 279\n",
            "reward highest for count 280\n",
            "reward highest for count 281\n",
            "reward highest for count 282\n",
            "600\n",
            "54.749125242233276 58.37196707725525\n",
            "-7.812556572757906\n",
            "1200\n",
            "52.912598848342896 59.34188365936279\n",
            "-11.900086777224665\n",
            "1800\n",
            "51.53248310089111 57.40096569061279\n",
            "-12.786354363134379\n",
            "2400\n",
            "50.749582052230835 56.468623876571655\n",
            "-12.90794009176517\n",
            "3000\n",
            "50.03914833068848 55.61178922653198\n",
            "-12.865860814047608\n",
            "3600\n",
            "49.877938628196716 55.01980185508728\n",
            "-12.777055311469475\n",
            "  7200/86400: episode: 2, duration: 30.268s, episode steps: 3600, steps per second: 119, episode reward: -40375.680, mean reward: -11.215 [-17.000,  1.000], mean action: 0.547 [0.493, 0.604],  loss: 48.316273, mean_q: -9.167835\n",
            "reward highest for count 78\n",
            "reward highest for count 79\n",
            "reward highest for count 80\n",
            "reward highest for count 81\n",
            "reward highest for count 82\n",
            "reward highest for count 83\n",
            "reward highest for count 84\n",
            "reward highest for count 85\n",
            "reward highest for count 86\n",
            "reward highest for count 87\n",
            "reward highest for count 88\n",
            "reward highest for count 89\n",
            "600\n",
            "47.07987308502197 48.3015775680542\n",
            "-17.550974329356883\n",
            "1200\n",
            "46.90835475921631 46.32505178451538\n",
            "-21.88937188319524\n",
            "1800\n",
            "44.837209582328796 46.64754867553711\n",
            "-22.93181935835397\n",
            "2400\n",
            "41.355112195014954 44.94573771953583\n",
            "-22.9274626705283\n",
            "3000\n",
            "38.97473216056824 44.64925527572632\n",
            "-22.068374285026195\n",
            "3600\n",
            "37.43515610694885 43.90130341053009\n",
            "-20.559001405839687\n",
            " 10800/86400: episode: 3, duration: 30.645s, episode steps: 3600, steps per second: 117, episode reward: -72044.970, mean reward: -20.012 [-22.983,  1.000], mean action: 0.452 [0.372, 0.520],  loss: 19.881674, mean_q: -27.796818\n",
            "reward highest for count 168\n",
            "reward highest for count 169\n",
            "reward highest for count 170\n",
            "reward highest for count 171\n",
            "reward highest for count 172\n",
            "reward highest for count 173\n",
            "reward highest for count 174\n",
            "reward highest for count 175\n",
            "reward highest for count 176\n",
            "reward highest for count 177\n",
            "reward highest for count 178\n",
            "reward highest for count 179\n",
            "reward highest for count 180\n",
            "reward highest for count 181\n",
            "reward highest for count 182\n",
            "reward highest for count 183\n",
            "reward highest for count 184\n",
            "reward highest for count 185\n",
            "reward highest for count 186\n",
            "600\n",
            "48.61678183078766 42.2795295715332\n",
            "-11.515149836476589\n",
            "1200\n",
            "45.39070129394531 40.32956063747406\n",
            "-15.679563329450858\n",
            "1800\n",
            "44.26772594451904 39.975517988204956\n",
            "-16.760185182893466\n",
            "2400\n",
            "43.49227845668793 38.79687488079071\n",
            "-17.016560837332392\n",
            "3000\n",
            "42.536550760269165 37.33217120170593\n",
            "-16.964588192837674\n",
            "3600\n",
            "41.797491908073425 36.64279878139496\n",
            "-16.880686722757808\n",
            " 14400/86400: episode: 4, duration: 30.208s, episode steps: 3600, steps per second: 119, episode reward: -52808.715, mean reward: -14.669 [-17.022,  1.000], mean action: 0.426 [0.366, 0.525],  loss: 9.623083, mean_q: -48.802673\n",
            "600\n",
            "41.32912456989288 42.5226092338562\n",
            "3.056003161947501\n",
            "reward highest for count 861\n",
            "reward highest for count 862\n",
            "reward highest for count 863\n",
            "reward highest for count 864\n",
            "reward highest for count 865\n",
            "reward highest for count 866\n",
            "reward highest for count 867\n",
            "reward highest for count 868\n",
            "reward highest for count 869\n",
            "reward highest for count 870\n",
            "reward highest for count 871\n",
            "reward highest for count 872\n",
            "reward highest for count 873\n",
            "reward highest for count 874\n",
            "reward highest for count 875\n",
            "reward highest for count 876\n",
            "reward highest for count 877\n",
            "reward highest for count 878\n",
            "reward highest for count 879\n",
            "reward highest for count 880\n",
            "reward highest for count 881\n",
            "reward highest for count 882\n",
            "reward highest for count 883\n",
            "reward highest for count 884\n",
            "reward highest for count 885\n",
            "reward highest for count 886\n",
            "reward highest for count 887\n",
            "reward highest for count 888\n",
            "reward highest for count 889\n",
            "reward highest for count 890\n",
            "reward highest for count 891\n",
            "reward highest for count 892\n",
            "reward highest for count 893\n",
            "reward highest for count 894\n",
            "reward highest for count 895\n",
            "reward highest for count 896\n",
            "reward highest for count 897\n",
            "reward highest for count 898\n",
            "reward highest for count 899\n",
            "reward highest for count 900\n",
            "reward highest for count 901\n",
            "reward highest for count 902\n",
            "reward highest for count 903\n",
            "reward highest for count 904\n",
            "reward highest for count 905\n",
            "reward highest for count 906\n",
            "reward highest for count 907\n",
            "reward highest for count 908\n",
            "reward highest for count 909\n",
            "reward highest for count 910\n",
            "reward highest for count 911\n",
            "reward highest for count 912\n",
            "reward highest for count 913\n",
            "reward highest for count 914\n",
            "reward highest for count 915\n",
            "reward highest for count 916\n",
            "reward highest for count 917\n",
            "reward highest for count 918\n",
            "reward highest for count 919\n",
            "reward highest for count 920\n",
            "reward highest for count 921\n",
            "reward highest for count 922\n",
            "reward highest for count 923\n",
            "reward highest for count 924\n",
            "reward highest for count 925\n",
            "reward highest for count 926\n",
            "reward highest for count 927\n",
            "reward highest for count 928\n",
            "reward highest for count 929\n",
            "reward highest for count 930\n",
            "reward highest for count 931\n",
            "reward highest for count 932\n",
            "reward highest for count 933\n",
            "reward highest for count 934\n",
            "reward highest for count 935\n",
            "reward highest for count 936\n",
            "reward highest for count 937\n",
            "reward highest for count 938\n",
            "reward highest for count 939\n",
            "reward highest for count 940\n",
            "reward highest for count 941\n",
            "reward highest for count 942\n",
            "reward highest for count 943\n",
            "reward highest for count 944\n",
            "reward highest for count 945\n",
            "reward highest for count 946\n",
            "reward highest for count 947\n",
            "reward highest for count 948\n",
            "reward highest for count 949\n",
            "reward highest for count 950\n",
            "reward highest for count 951\n",
            "reward highest for count 952\n",
            "reward highest for count 953\n",
            "reward highest for count 954\n",
            "reward highest for count 955\n",
            "reward highest for count 956\n",
            "reward highest for count 957\n",
            "reward highest for count 958\n",
            "reward highest for count 959\n",
            "reward highest for count 960\n",
            "reward highest for count 961\n",
            "reward highest for count 962\n",
            "reward highest for count 963\n",
            "reward highest for count 964\n",
            "reward highest for count 965\n",
            "reward highest for count 966\n",
            "reward highest for count 967\n",
            "reward highest for count 968\n",
            "reward highest for count 969\n",
            "reward highest for count 970\n",
            "reward highest for count 971\n",
            "reward highest for count 972\n",
            "reward highest for count 973\n",
            "reward highest for count 974\n",
            "reward highest for count 975\n",
            "reward highest for count 976\n",
            "reward highest for count 977\n",
            "reward highest for count 978\n",
            "reward highest for count 979\n",
            "reward highest for count 980\n",
            "reward highest for count 981\n",
            "reward highest for count 982\n",
            "reward highest for count 983\n",
            "reward highest for count 984\n",
            "reward highest for count 985\n",
            "reward highest for count 986\n",
            "reward highest for count 987\n",
            "reward highest for count 988\n",
            "reward highest for count 989\n",
            "reward highest for count 990\n",
            "reward highest for count 991\n",
            "reward highest for count 992\n",
            "reward highest for count 993\n",
            "reward highest for count 994\n",
            "reward highest for count 995\n",
            "reward highest for count 996\n",
            "reward highest for count 997\n",
            "reward highest for count 998\n",
            "reward highest for count 999\n",
            "reward highest for count 1000\n",
            "reward highest for count 1001\n",
            "reward highest for count 1002\n",
            "reward highest for count 1003\n",
            "reward highest for count 1004\n",
            "reward highest for count 1005\n",
            "reward highest for count 1006\n",
            "reward highest for count 1007\n",
            "reward highest for count 1008\n",
            "reward highest for count 1009\n",
            "reward highest for count 1010\n",
            "reward highest for count 1011\n",
            "reward highest for count 1012\n",
            "reward highest for count 1013\n",
            "reward highest for count 1014\n",
            "reward highest for count 1015\n",
            "reward highest for count 1016\n",
            "reward highest for count 1017\n",
            "reward highest for count 1018\n",
            "reward highest for count 1019\n",
            "reward highest for count 1020\n",
            "reward highest for count 1021\n",
            "reward highest for count 1022\n",
            "reward highest for count 1023\n",
            "reward highest for count 1024\n",
            "reward highest for count 1025\n",
            "reward highest for count 1026\n",
            "reward highest for count 1027\n",
            "reward highest for count 1028\n",
            "reward highest for count 1029\n",
            "reward highest for count 1030\n",
            "reward highest for count 1031\n",
            "reward highest for count 1032\n",
            "reward highest for count 1033\n",
            "reward highest for count 1034\n",
            "reward highest for count 1035\n",
            "reward highest for count 1036\n",
            "reward highest for count 1037\n",
            "reward highest for count 1038\n",
            "reward highest for count 1039\n",
            "reward highest for count 1040\n",
            "reward highest for count 1041\n",
            "reward highest for count 1042\n",
            "reward highest for count 1043\n",
            "reward highest for count 1044\n",
            "reward highest for count 1045\n",
            "1200\n",
            "39.75363075733185 40.178242325782776\n",
            "-1.0491870044259954\n",
            "1800\n",
            "38.793858885765076 39.01760280132294\n",
            "-1.3500373962261634\n",
            "2400\n",
            "37.827545404434204 38.40627670288086\n",
            "-0.8366207446247813\n",
            "3000\n",
            "37.64194846153259 37.886637449264526\n",
            "-0.5935770781357377\n",
            "reward highest for count 3422\n",
            "reward highest for count 3423\n",
            "reward highest for count 3424\n",
            "reward highest for count 3425\n",
            "reward highest for count 3426\n",
            "reward highest for count 3427\n",
            "reward highest for count 3428\n",
            "reward highest for count 3429\n",
            "reward highest for count 3430\n",
            "reward highest for count 3431\n",
            "reward highest for count 3432\n",
            "reward highest for count 3433\n",
            "reward highest for count 3434\n",
            "reward highest for count 3435\n",
            "reward highest for count 3436\n",
            "reward highest for count 3437\n",
            "reward highest for count 3438\n",
            "reward highest for count 3439\n",
            "reward highest for count 3440\n",
            "reward highest for count 3441\n",
            "reward highest for count 3442\n",
            "reward highest for count 3443\n",
            "reward highest for count 3444\n",
            "reward highest for count 3445\n",
            "reward highest for count 3446\n",
            "reward highest for count 3447\n",
            "reward highest for count 3448\n",
            "reward highest for count 3449\n",
            "reward highest for count 3450\n",
            "reward highest for count 3451\n",
            "reward highest for count 3452\n",
            "reward highest for count 3453\n",
            "reward highest for count 3454\n",
            "reward highest for count 3455\n",
            "reward highest for count 3456\n",
            "reward highest for count 3457\n",
            "reward highest for count 3458\n",
            "reward highest for count 3459\n",
            "reward highest for count 3460\n",
            "reward highest for count 3461\n",
            "reward highest for count 3462\n",
            "reward highest for count 3463\n",
            "reward highest for count 3464\n",
            "reward highest for count 3465\n",
            "reward highest for count 3466\n",
            "reward highest for count 3467\n",
            "reward highest for count 3468\n",
            "reward highest for count 3469\n",
            "reward highest for count 3470\n",
            "reward highest for count 3471\n",
            "reward highest for count 3472\n",
            "reward highest for count 3473\n",
            "reward highest for count 3474\n",
            "reward highest for count 3475\n",
            "reward highest for count 3476\n",
            "reward highest for count 3477\n",
            "reward highest for count 3478\n",
            "reward highest for count 3479\n",
            "reward highest for count 3480\n",
            "reward highest for count 3481\n",
            "reward highest for count 3482\n",
            "reward highest for count 3483\n",
            "reward highest for count 3484\n",
            "reward highest for count 3485\n",
            "reward highest for count 3486\n",
            "reward highest for count 3487\n",
            "reward highest for count 3488\n",
            "reward highest for count 3489\n",
            "reward highest for count 3490\n",
            "reward highest for count 3491\n",
            "reward highest for count 3492\n",
            "reward highest for count 3493\n",
            "reward highest for count 3494\n",
            "reward highest for count 3495\n",
            "reward highest for count 3496\n",
            "reward highest for count 3497\n",
            "reward highest for count 3498\n",
            "reward highest for count 3499\n",
            "reward highest for count 3500\n",
            "reward highest for count 3501\n",
            "reward highest for count 3502\n",
            "reward highest for count 3503\n",
            "reward highest for count 3504\n",
            "reward highest for count 3505\n",
            "reward highest for count 3506\n",
            "reward highest for count 3507\n",
            "reward highest for count 3508\n",
            "reward highest for count 3509\n",
            "reward highest for count 3510\n",
            "reward highest for count 3511\n",
            "reward highest for count 3512\n",
            "reward highest for count 3513\n",
            "reward highest for count 3514\n",
            "reward highest for count 3515\n",
            "reward highest for count 3516\n",
            "reward highest for count 3517\n",
            "reward highest for count 3518\n",
            "reward highest for count 3519\n",
            "reward highest for count 3520\n",
            "reward highest for count 3521\n",
            "reward highest for count 3522\n",
            "reward highest for count 3523\n",
            "reward highest for count 3524\n",
            "reward highest for count 3525\n",
            "reward highest for count 3526\n",
            "reward highest for count 3527\n",
            "reward highest for count 3528\n",
            "reward highest for count 3529\n",
            "reward highest for count 3530\n",
            "reward highest for count 3531\n",
            "reward highest for count 3532\n",
            "reward highest for count 3533\n",
            "reward highest for count 3534\n",
            "reward highest for count 3535\n",
            "reward highest for count 3536\n",
            "reward highest for count 3537\n",
            "reward highest for count 3538\n",
            "reward highest for count 3539\n",
            "reward highest for count 3540\n",
            "reward highest for count 3541\n",
            "reward highest for count 3542\n",
            "reward highest for count 3543\n",
            "reward highest for count 3544\n",
            "reward highest for count 3545\n",
            "reward highest for count 3546\n",
            "reward highest for count 3547\n",
            "reward highest for count 3548\n",
            "reward highest for count 3549\n",
            "reward highest for count 3550\n",
            "reward highest for count 3551\n",
            "reward highest for count 3552\n",
            "reward highest for count 3553\n",
            "reward highest for count 3554\n",
            "reward highest for count 3555\n",
            "reward highest for count 3556\n",
            "reward highest for count 3557\n",
            "reward highest for count 3558\n",
            "reward highest for count 3559\n",
            "reward highest for count 3560\n",
            "reward highest for count 3561\n",
            "reward highest for count 3562\n",
            "reward highest for count 3563\n",
            "reward highest for count 3564\n",
            "reward highest for count 3565\n",
            "reward highest for count 3566\n",
            "reward highest for count 3567\n",
            "reward highest for count 3568\n",
            "reward highest for count 3569\n",
            "reward highest for count 3570\n",
            "reward highest for count 3571\n",
            "reward highest for count 3572\n",
            "reward highest for count 3573\n",
            "reward highest for count 3574\n",
            "reward highest for count 3575\n",
            "reward highest for count 3576\n",
            "reward highest for count 3577\n",
            "reward highest for count 3578\n",
            "reward highest for count 3579\n",
            "reward highest for count 3580\n",
            "reward highest for count 3581\n",
            "reward highest for count 3582\n",
            "reward highest for count 3583\n",
            "reward highest for count 3584\n",
            "reward highest for count 3585\n",
            "reward highest for count 3586\n",
            "reward highest for count 3587\n",
            "reward highest for count 3588\n",
            "reward highest for count 3589\n",
            "reward highest for count 3590\n",
            "reward highest for count 3591\n",
            "reward highest for count 3592\n",
            "reward highest for count 3593\n",
            "reward highest for count 3594\n",
            "reward highest for count 3595\n",
            "reward highest for count 3596\n",
            "reward highest for count 3597\n",
            "reward highest for count 3598\n",
            "reward highest for count 3599\n",
            "3600\n",
            "37.247711420059204 36.724114418029785\n",
            "-0.1588729751769904\n",
            "reward highest for count 3600\n",
            " 18000/86400: episode: 5, duration: 30.982s, episode steps: 3600, steps per second: 116, episode reward: -9063.788, mean reward: -2.518 [-26.000,  1.000], mean action: 0.396 [0.367, 0.459],  loss: 14.175215, mean_q: -59.986435\n",
            "reward highest for count 224\n",
            "reward highest for count 225\n",
            "reward highest for count 226\n",
            "reward highest for count 227\n",
            "reward highest for count 228\n",
            "reward highest for count 229\n",
            "reward highest for count 230\n",
            "reward highest for count 231\n",
            "reward highest for count 232\n",
            "reward highest for count 233\n",
            "reward highest for count 234\n",
            "reward highest for count 235\n",
            "reward highest for count 236\n",
            "reward highest for count 237\n",
            "reward highest for count 238\n",
            "reward highest for count 239\n",
            "reward highest for count 240\n",
            "reward highest for count 241\n",
            "reward highest for count 242\n",
            "reward highest for count 243\n",
            "reward highest for count 244\n",
            "reward highest for count 245\n",
            "reward highest for count 246\n",
            "reward highest for count 247\n",
            "reward highest for count 248\n",
            "600\n",
            "37.79817521572113 38.230013847351074\n",
            "-7.269311205302387\n",
            "1200\n",
            "35.01771688461304 35.84640622138977\n",
            "-8.498041223136482\n",
            "1800\n",
            "34.1126412153244 34.4498336315155\n",
            "-8.164404913639963\n",
            "2400\n",
            "33.4060400724411 33.75031054019928\n",
            "-7.400660313974029\n",
            "3000\n",
            "32.97026753425598 33.08698832988739\n",
            "-6.775212575775505\n",
            "3600\n",
            "32.32618272304535 33.35210084915161\n",
            "-5.87327387329773\n",
            " 21600/86400: episode: 6, duration: 30.604s, episode steps: 3600, steps per second: 118, episode reward: -25831.120, mean reward: -7.175 [-14.000,  1.000], mean action: 0.354 [0.323, 0.439],  loss: 15.652472, mean_q: -54.616581\n",
            "reward highest for count 173\n",
            "reward highest for count 174\n",
            "reward highest for count 175\n",
            "reward highest for count 176\n",
            "reward highest for count 177\n",
            "reward highest for count 178\n",
            "reward highest for count 179\n",
            "reward highest for count 180\n",
            "reward highest for count 181\n",
            "reward highest for count 182\n",
            "reward highest for count 183\n",
            "reward highest for count 184\n",
            "reward highest for count 185\n",
            "reward highest for count 186\n",
            "reward highest for count 187\n",
            "reward highest for count 188\n",
            "reward highest for count 189\n",
            "reward highest for count 190\n",
            "reward highest for count 191\n",
            "reward highest for count 192\n",
            "reward highest for count 193\n",
            "reward highest for count 194\n",
            "reward highest for count 195\n",
            "reward highest for count 196\n",
            "reward highest for count 197\n",
            "reward highest for count 198\n",
            "reward highest for count 199\n",
            "600\n",
            "32.01936483383179 32.45938718318939\n",
            "-5.13585432262828\n",
            "1200\n",
            "30.18306791782379 30.42047619819641\n",
            "-4.858509339061072\n",
            "1800\n",
            "29.507052898406982 29.703131318092346\n",
            "-4.850635482691089\n",
            "2400\n",
            "28.580304980278015 28.850704431533813\n",
            "-4.969880876409441\n",
            "3000\n",
            "27.52959430217743 27.955472469329834\n",
            "-4.948354709091792\n",
            "3600\n",
            "26.639649271965027 27.23202109336853\n",
            "-4.898233399559061\n",
            " 25200/86400: episode: 7, duration: 30.398s, episode steps: 3600, steps per second: 118, episode reward: -17192.600, mean reward: -4.776 [-10.000,  1.000], mean action: 0.302 [0.266, 0.396],  loss: 5.817105, mean_q: -49.391636\n",
            "600\n",
            "30.660131573677063 32.53135681152344\n",
            "5.837536213048143\n",
            "1200\n",
            "28.50571870803833 30.23799955844879\n",
            "4.940721206471\n",
            "1800\n",
            "26.91645324230194 29.173392057418823\n",
            "4.317219884439954\n",
            "2400\n",
            "26.010039448738098 28.237977623939514\n",
            "4.154889787635142\n",
            "3000\n",
            "25.25809109210968 27.376097440719604\n",
            "4.1601011705690745\n",
            "3600\n",
            "24.50043112039566 26.559314131736755\n",
            "4.210633458156654\n",
            " 28800/86400: episode: 8, duration: 30.476s, episode steps: 3600, steps per second: 118, episode reward: -19290.638, mean reward: -5.359 [-19.000, -4.145], mean action: 0.289 [0.245, 0.388],  loss: 2.848123, mean_q: -48.460148\n",
            "reward highest for count 263\n",
            "reward highest for count 264\n",
            "reward highest for count 265\n",
            "reward highest for count 266\n",
            "reward highest for count 267\n",
            "reward highest for count 268\n",
            "reward highest for count 269\n",
            "reward highest for count 270\n",
            "reward highest for count 271\n",
            "reward highest for count 272\n",
            "reward highest for count 273\n",
            "reward highest for count 274\n",
            "reward highest for count 275\n",
            "reward highest for count 276\n",
            "reward highest for count 277\n",
            "reward highest for count 278\n",
            "reward highest for count 279\n",
            "reward highest for count 280\n",
            "reward highest for count 281\n",
            "reward highest for count 282\n",
            "reward highest for count 283\n",
            "reward highest for count 284\n",
            "reward highest for count 285\n",
            "reward highest for count 286\n",
            "reward highest for count 287\n",
            "reward highest for count 288\n",
            "reward highest for count 289\n",
            "reward highest for count 290\n",
            "reward highest for count 291\n",
            "reward highest for count 292\n",
            "reward highest for count 293\n",
            "reward highest for count 294\n",
            "reward highest for count 295\n",
            "reward highest for count 296\n",
            "reward highest for count 297\n",
            "reward highest for count 298\n",
            "reward highest for count 299\n",
            "reward highest for count 300\n",
            "reward highest for count 301\n",
            "reward highest for count 302\n",
            "reward highest for count 303\n",
            "reward highest for count 304\n",
            "reward highest for count 305\n",
            "reward highest for count 306\n",
            "reward highest for count 307\n",
            "reward highest for count 308\n",
            "reward highest for count 309\n",
            "reward highest for count 310\n",
            "reward highest for count 311\n",
            "reward highest for count 312\n",
            "reward highest for count 313\n",
            "reward highest for count 314\n",
            "reward highest for count 315\n",
            "reward highest for count 316\n",
            "reward highest for count 317\n",
            "reward highest for count 318\n",
            "reward highest for count 319\n",
            "reward highest for count 320\n",
            "reward highest for count 321\n",
            "600\n",
            "24.1907000541687 24.318857491016388\n",
            "-3.457485992391902\n",
            "1200\n",
            "20.030730962753296 20.108449459075928\n",
            "-5.835486921424621\n",
            "1800\n",
            "18.62451881170273 18.28320324420929\n",
            "-6.527200103192438\n",
            "2400\n",
            "17.908121645450592 17.601995170116425\n",
            "-6.710261939947209\n",
            "3000\n",
            "17.320875823497772 16.474197804927826\n",
            "-6.642580398589107\n",
            "3600\n",
            "16.66145622730255 15.713754296302795\n",
            "-6.515867242125115\n",
            " 32400/86400: episode: 9, duration: 30.516s, episode steps: 3600, steps per second: 118, episode reward: -20114.637, mean reward: -5.587 [-7.000,  1.000], mean action: 0.201 [0.157, 0.322],  loss: 2.421741, mean_q: -45.056774\n",
            "reward highest for count 1\n",
            " 32401/86400: episode: 10, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward:  1.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.190 [0.183, 0.197],  loss: 2.059487, mean_q: -45.309055\n",
            "reward highest for count 520\n",
            "reward highest for count 521\n",
            "reward highest for count 522\n",
            "reward highest for count 523\n",
            "reward highest for count 524\n",
            "reward highest for count 525\n",
            "reward highest for count 526\n",
            "reward highest for count 527\n",
            "reward highest for count 528\n",
            "reward highest for count 529\n",
            "reward highest for count 530\n",
            "reward highest for count 531\n",
            "reward highest for count 532\n",
            "reward highest for count 533\n",
            "reward highest for count 534\n",
            "reward highest for count 535\n",
            "reward highest for count 536\n",
            "reward highest for count 537\n",
            "reward highest for count 538\n",
            "reward highest for count 539\n",
            "reward highest for count 540\n",
            "reward highest for count 541\n",
            "reward highest for count 542\n",
            "reward highest for count 543\n",
            "reward highest for count 544\n",
            "reward highest for count 545\n",
            "reward highest for count 546\n",
            "reward highest for count 547\n",
            "reward highest for count 548\n",
            "reward highest for count 549\n",
            "reward highest for count 550\n",
            "reward highest for count 551\n",
            "reward highest for count 552\n",
            "reward highest for count 553\n",
            "reward highest for count 554\n",
            "reward highest for count 555\n",
            "reward highest for count 556\n",
            "reward highest for count 557\n",
            "reward highest for count 558\n",
            "reward highest for count 559\n",
            "reward highest for count 560\n",
            "reward highest for count 561\n",
            "reward highest for count 562\n",
            "reward highest for count 563\n",
            "reward highest for count 564\n",
            "reward highest for count 565\n",
            "reward highest for count 566\n",
            "reward highest for count 567\n",
            "reward highest for count 568\n",
            "reward highest for count 569\n",
            "reward highest for count 570\n",
            "reward highest for count 571\n",
            "reward highest for count 572\n",
            "reward highest for count 573\n",
            "reward highest for count 574\n",
            "reward highest for count 575\n",
            "reward highest for count 576\n",
            "reward highest for count 577\n",
            "reward highest for count 578\n",
            "reward highest for count 579\n",
            "reward highest for count 580\n",
            "reward highest for count 581\n",
            "reward highest for count 582\n",
            "reward highest for count 583\n",
            "reward highest for count 584\n",
            "reward highest for count 585\n",
            "reward highest for count 586\n",
            "reward highest for count 587\n",
            "reward highest for count 588\n",
            "reward highest for count 589\n",
            "reward highest for count 590\n",
            "reward highest for count 591\n",
            "reward highest for count 592\n",
            "reward highest for count 593\n",
            "reward highest for count 594\n",
            "reward highest for count 595\n",
            "reward highest for count 596\n",
            "reward highest for count 597\n",
            "reward highest for count 598\n",
            "reward highest for count 599\n",
            "600\n",
            "21.945564448833466 23.38407039642334\n",
            "-0.13771194495140548\n",
            "reward highest for count 600\n",
            "reward highest for count 601\n",
            "reward highest for count 602\n",
            "reward highest for count 603\n",
            "reward highest for count 604\n",
            "reward highest for count 605\n",
            "reward highest for count 606\n",
            "reward highest for count 607\n",
            "reward highest for count 608\n",
            "reward highest for count 609\n",
            "reward highest for count 610\n",
            "reward highest for count 611\n",
            "reward highest for count 612\n",
            "reward highest for count 613\n",
            "reward highest for count 614\n",
            "reward highest for count 615\n",
            "reward highest for count 616\n",
            "reward highest for count 617\n",
            "reward highest for count 618\n",
            "reward highest for count 619\n",
            "reward highest for count 620\n",
            "reward highest for count 621\n",
            "reward highest for count 622\n",
            "reward highest for count 623\n",
            "reward highest for count 624\n",
            "reward highest for count 625\n",
            "reward highest for count 626\n",
            "reward highest for count 627\n",
            "reward highest for count 628\n",
            "reward highest for count 629\n",
            "reward highest for count 630\n",
            "reward highest for count 631\n",
            "reward highest for count 632\n",
            "reward highest for count 633\n",
            "reward highest for count 634\n",
            "reward highest for count 635\n",
            "reward highest for count 636\n",
            "reward highest for count 637\n",
            "reward highest for count 638\n",
            "reward highest for count 639\n",
            "reward highest for count 640\n",
            "reward highest for count 641\n",
            "reward highest for count 642\n",
            "reward highest for count 643\n",
            "reward highest for count 644\n",
            "reward highest for count 645\n",
            "reward highest for count 646\n",
            "reward highest for count 647\n",
            "reward highest for count 648\n",
            "reward highest for count 649\n",
            "reward highest for count 650\n",
            "reward highest for count 651\n",
            "reward highest for count 652\n",
            "1200\n",
            "18.106505274772644 19.547435641288757\n",
            "-2.5379789452430046\n",
            "1800\n",
            "16.071277856826782 17.951588332653046\n",
            "-3.182126367545038\n",
            "2400\n",
            "15.036222338676453 17.076171934604645\n",
            "-3.2910642343628744\n",
            "3000\n",
            "14.301665127277374 16.413012146949768\n",
            "-3.2230556069302665\n",
            "3600\n",
            "13.877396285533905 16.034507751464844\n",
            "-3.1108590512171936\n",
            " 36001/86400: episode: 11, duration: 30.692s, episode steps: 3600, steps per second: 117, episode reward: -10717.075, mean reward: -2.977 [-10.000,  1.000], mean action: 0.187 [0.138, 0.310],  loss: 2.080488, mean_q: -44.537899\n",
            "reward highest for count 120\n",
            "reward highest for count 121\n",
            "reward highest for count 122\n",
            "reward highest for count 123\n",
            "reward highest for count 124\n",
            "reward highest for count 125\n",
            "reward highest for count 126\n",
            "reward highest for count 127\n",
            "reward highest for count 128\n",
            "reward highest for count 129\n",
            "reward highest for count 130\n",
            "reward highest for count 131\n",
            "reward highest for count 132\n",
            "reward highest for count 133\n",
            "reward highest for count 134\n",
            "reward highest for count 135\n",
            "reward highest for count 136\n",
            "reward highest for count 137\n",
            "reward highest for count 138\n",
            "reward highest for count 139\n",
            "reward highest for count 140\n",
            "reward highest for count 141\n",
            "reward highest for count 142\n",
            "reward highest for count 143\n",
            "reward highest for count 144\n",
            "reward highest for count 145\n",
            "reward highest for count 146\n",
            "reward highest for count 147\n",
            "reward highest for count 148\n",
            "reward highest for count 149\n",
            "reward highest for count 150\n",
            "reward highest for count 151\n",
            "reward highest for count 152\n",
            "reward highest for count 153\n",
            "reward highest for count 154\n",
            "reward highest for count 155\n",
            "reward highest for count 156\n",
            "reward highest for count 157\n",
            "reward highest for count 158\n",
            "reward highest for count 159\n",
            "reward highest for count 160\n",
            "reward highest for count 161\n",
            "reward highest for count 162\n",
            "reward highest for count 163\n",
            "reward highest for count 164\n",
            "reward highest for count 165\n",
            "reward highest for count 166\n",
            "reward highest for count 167\n",
            "reward highest for count 168\n",
            "reward highest for count 169\n",
            "reward highest for count 170\n",
            "reward highest for count 171\n",
            "reward highest for count 172\n",
            "reward highest for count 173\n",
            "reward highest for count 174\n",
            "reward highest for count 175\n",
            "reward highest for count 176\n",
            "600\n",
            "12.891624867916107 12.103676795959473\n",
            "-4.662355379546877\n",
            "1200\n",
            "9.742672741413116 9.29553136229515\n",
            "-6.367423963332609\n",
            "1800\n",
            "8.79802256822586 8.055099099874496\n",
            "-6.5869803212198725\n",
            "2400\n",
            "8.294632285833359 7.2118885815143585\n",
            "-6.417049381938909\n",
            "3000\n",
            "7.908739149570465 6.981661915779114\n",
            "-6.061212524527129\n",
            "3600\n",
            "7.762622833251953 6.276164948940277\n",
            "-5.630426594862577\n",
            " 39601/86400: episode: 12, duration: 30.779s, episode steps: 3600, steps per second: 117, episode reward: -19879.187, mean reward: -5.522 [-6.601,  1.000], mean action: 0.098 [0.062, 0.201],  loss: 2.039225, mean_q: -44.993202\n",
            "600\n",
            "16.462121903896332 16.28032773733139\n",
            "5.669386867498812\n",
            "1200\n",
            "13.82991224527359 13.71651440858841\n",
            "3.3114734581937846\n",
            "1800\n",
            "12.286659330129623 12.138774991035461\n",
            "2.5703132509677005\n",
            "2400\n",
            "11.564639955759048 11.109597980976105\n",
            "2.4171241272814044\n",
            "3000\n",
            "10.940633714199066 10.243892669677734\n",
            "2.551464299972377\n",
            "3600\n",
            "10.619224607944489 9.755904227495193\n",
            "2.7628505451957963\n",
            " 43201/86400: episode: 13, duration: 30.385s, episode steps: 3600, steps per second: 118, episode reward: -14331.234, mean reward: -3.981 [-14.000, -2.414], mean action: 0.134 [0.096, 0.240],  loss: 3.602524, mean_q: -43.083954\n",
            "600\n",
            "10.775692760944366 11.881809681653976\n",
            "3.2304536563085406\n",
            "1200\n",
            "8.567304164171219 9.39907431602478\n",
            "1.6344287227135155\n",
            "1800\n",
            "7.433300465345383 8.291514962911606\n",
            "1.6062921824824237\n",
            "2400\n",
            "7.321827858686447 7.99461305141449\n",
            "1.6563000874290879\n",
            "3000\n",
            "6.744789332151413 7.258236408233643\n",
            "2.077392919090311\n",
            "3600\n",
            "6.458368897438049 6.954578310251236\n",
            "2.5048124844777924\n",
            " 46801/86400: episode: 14, duration: 30.527s, episode steps: 3600, steps per second: 118, episode reward: -9179.022, mean reward: -2.550 [-9.000, -1.551], mean action: 0.090 [0.064, 0.178],  loss: 2.679889, mean_q: -39.411705\n",
            "600\n",
            "7.238217443227768 9.608563035726547\n",
            "5.626294994213147\n",
            "1200\n",
            "6.0225144028663635 8.20571780204773\n",
            "4.946380794171144\n",
            "1800\n",
            "5.412173271179199 7.379743456840515\n",
            "4.767256125732612\n",
            "2400\n",
            "4.708996042609215 6.8109095096588135\n",
            "4.917180274794969\n",
            "3000\n",
            "4.21307347714901 6.755387037992477\n",
            "5.18039731771794\n",
            "3600\n",
            "4.344730824232101 6.852125376462936\n",
            "5.428110541326443\n",
            " 50401/86400: episode: 15, duration: 30.371s, episode steps: 3600, steps per second: 119, episode reward: -19372.152, mean reward: -5.381 [-9.000, -4.767], mean action: 0.070 [0.040, 0.143],  loss: 2.694173, mean_q: -33.595867\n",
            "600\n",
            "9.290042519569397 9.129436314105988\n",
            "19.681342573504445\n",
            "1200\n",
            "7.610780745744705 7.564546167850494\n",
            "18.6105376827256\n",
            "1800\n",
            "6.497270613908768 6.729424744844437\n",
            "18.454344953605148\n",
            "2400\n",
            "6.403601169586182 6.824718415737152\n",
            "18.48488578355392\n",
            "3000\n",
            "6.195361912250519 6.109054759144783\n",
            "18.588094888077862\n",
            "3600\n",
            "5.737831816077232 5.505257472395897\n",
            "18.921426535461325\n",
            " 54001/86400: episode: 16, duration: 30.489s, episode steps: 3600, steps per second: 118, episode reward: -68912.902, mean reward: -19.142 [-24.000, -18.435], mean action: 0.076 [0.055, 0.147],  loss: 26.261654, mean_q: -35.079330\n",
            "600\n",
            "4.715614393353462 5.484392121434212\n",
            "12.061191578830819\n",
            "1200\n",
            "4.321427643299103 4.310111701488495\n",
            "11.732139619612578\n",
            "1800\n",
            "3.3794302493333817 3.7440024316310883\n",
            "11.658333934032855\n",
            "2400\n",
            "3.236669674515724 3.614203631877899\n",
            "11.683241887554127\n",
            "3000\n",
            "3.0236735939979553 3.187556192278862\n",
            "11.74280845323774\n",
            "3600\n",
            "2.825658768415451 3.327047824859619\n",
            "11.827371340635324\n",
            " 57601/86400: episode: 17, duration: 30.582s, episode steps: 3600, steps per second: 118, episode reward: -42714.531, mean reward: -11.865 [-13.000, -11.653], mean action: 0.041 [0.027, 0.081],  loss: 25.642614, mean_q: -44.605797\n",
            "600\n",
            "3.9433367550373077 4.95523102581501\n",
            "20.437601256166463\n",
            "1200\n",
            "3.310108184814453 4.239917546510696\n",
            "20.21700387856702\n",
            "1800\n",
            "2.6644982397556305 3.990636393427849\n",
            "20.195879292663925\n",
            "2400\n",
            "2.5168759748339653 3.6332130432128906\n",
            "20.198334903240458\n",
            "3000\n",
            "2.223941683769226 3.4220773726701736\n",
            "20.234569567707986\n",
            "3600\n",
            "1.3816138729453087 3.235016018152237\n",
            "20.33525886735964\n",
            " 61201/86400: episode: 18, duration: 30.585s, episode steps: 3600, steps per second: 118, episode reward: -73130.445, mean reward: -20.314 [-21.000, -20.190], mean action: 0.036 [0.014, 0.079],  loss: 12.795380, mean_q: -59.068005\n",
            "600\n",
            "0.13124014949426055 1.9906546920537949\n",
            "0.9995394117310443\n",
            "1200\n",
            "0.5065464414656162 1.9336827099323273\n",
            "0.9986151545143667\n",
            "1800\n",
            "0.15514511615037918 2.0789768546819687\n",
            "0.9981891970173926\n",
            "2400\n",
            "-0.2543604001402855 1.814403384923935\n",
            "0.9980412813812034\n",
            "3000\n",
            "-0.8597943931818008 1.8307927995920181\n",
            "0.9920083636672352\n",
            "3600\n",
            "-0.3459558356553316 1.8845250830054283\n",
            "0.9899607003237882\n",
            " 64801/86400: episode: 19, duration: 30.219s, episode steps: 3600, steps per second: 119, episode reward: -3589.324, mean reward: -0.997 [-1.000, -0.990], mean action: 0.009 [-0.009, 0.024],  loss: 29.617161, mean_q: -68.805695\n",
            "600\n",
            "-0.7760864682495594 1.0583492927253246\n",
            "6.995758629567462\n",
            "1200\n",
            "-0.9905196726322174 0.9712958708405495\n",
            "6.991065497352158\n",
            "1800\n",
            "-0.7585869636386633 0.6485939491540194\n",
            "6.98809628749347\n",
            "2400\n",
            "-1.1923575773835182 0.5543848965317011\n",
            "6.9867642033743405\n",
            "3000\n",
            "-0.8110701106488705 0.8131252601742744\n",
            "6.983848455572248\n",
            "3600\n",
            "-0.6969838868826628 0.7983437739312649\n",
            "6.983886158927469\n",
            " 68401/86400: episode: 20, duration: 30.772s, episode steps: 3600, steps per second: 117, episode reward: -25162.459, mean reward: -6.990 [-7.000, -6.984], mean action: 0.000 [-0.012, 0.017],  loss: 26.639444, mean_q: -70.854286\n",
            "600\n",
            "-0.5910510197281837 1.0322016663849354\n",
            "3.9985802592025923\n",
            "1200\n",
            "-0.5969085730612278 1.175769604742527\n",
            "3.994415779015652\n",
            "1800\n",
            "-0.5063473712652922 1.3150287792086601\n",
            "3.992389078435407\n",
            "2400\n",
            "-0.8056350983679295 1.5946615487337112\n",
            "3.9877214009228883\n",
            "3000\n",
            "-0.8612345904111862 1.9306913018226624\n",
            "3.9787480781092164\n",
            "3600\n",
            "-0.9554880671203136 1.734531857073307\n",
            "3.969226541475283\n",
            " 72001/86400: episode: 21, duration: 30.400s, episode steps: 3600, steps per second: 118, episode reward: -14362.170, mean reward: -3.989 [-4.000, -3.969], mean action: 0.003 [-0.011, 0.020],  loss: 5.779302, mean_q: -66.006668\n",
            "600\n",
            "1.858947053551674 0.6128193344920874\n",
            "7.988649410919027\n",
            "1200\n",
            "2.024279162287712 0.5859644617885351\n",
            "7.971313780270313\n",
            "1800\n",
            "1.8491867929697037 0.29502147808671\n",
            "7.964340748333775\n",
            "2400\n",
            "2.05841101706028 0.4381971899420023\n",
            "7.963948644532781\n",
            "3000\n",
            "2.0930878818035126 0.3430426586419344\n",
            "7.958512639774941\n",
            "3600\n",
            "2.044724114239216 0.5514222662895918\n",
            "7.951879716786326\n",
            " 75601/86400: episode: 22, duration: 30.661s, episode steps: 3600, steps per second: 117, episode reward: -28694.139, mean reward: -7.971 [-8.000, -7.952], mean action: 0.012 [0.001, 0.022],  loss: 2.217006, mean_q: -65.082909\n",
            "600\n",
            "-2.3446807637810707 -0.9527855552732944\n",
            "4.0298489190913855\n",
            "1200\n",
            "-2.3443177342414856 -0.7351081352680922\n",
            "4.052862467897523\n",
            "1800\n",
            "-2.6618897914886475 -1.0897789150476456\n",
            "4.096484968691788\n",
            "2400\n",
            "-2.1890517324209213 -0.7981583476066589\n",
            "4.1233113275899935\n",
            "3000\n",
            "-2.6344656944274902 -0.8462315425276756\n",
            "4.128374920078471\n",
            "3600\n",
            "-2.3058850318193436 -0.8146019652485847\n",
            "4.145418897891949\n",
            " 79201/86400: episode: 23, duration: 30.193s, episode steps: 3600, steps per second: 119, episode reward: -14705.581, mean reward: -4.085 [-4.146, -4.000], mean action: -0.017 [-0.028, -0.005],  loss: 1.500461, mean_q: -63.677299\n",
            "600\n",
            "-0.10103557724505663 0.06298075895756483\n",
            "13.999943479072453\n",
            "1200\n",
            "0.0438722810940817 -0.061164714861661196\n",
            "13.99995283974985\n",
            "1800\n",
            "0.03896979324053973 -0.1547182211652398\n",
            "13.99997323486284\n",
            "2400\n",
            "-0.3664695890620351 -0.1023514661937952\n",
            "14.000069457003269\n",
            "3000\n",
            "-0.5267341621220112 -0.39291917346417904\n",
            "14.000720462150264\n",
            "3600\n",
            "-0.7540842052549124 -0.30665164813399315\n",
            "14.002346313363056\n",
            " 82801/86400: episode: 24, duration: 30.343s, episode steps: 3600, steps per second: 119, episode reward: -50401.073, mean reward: -14.000 [-14.002, -14.000], mean action: -0.002 [-0.009, 0.007],  loss: 4.126493, mean_q: -66.177155\n",
            "reward highest for count 1\n",
            " 82802/86400: episode: 25, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward:  1.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.001 [-0.005, 0.007],  loss: 5.743731, mean_q: -71.281036\n",
            "600\n",
            "0.8264273405075073 -0.4384285304695368\n",
            "25.0029075881893\n",
            "1200\n",
            "1.1309094727039337 -0.48646838404238224\n",
            "25.006448908151256\n",
            "1800\n",
            "1.0963564738631248 -0.5460770800709724\n",
            "25.010493316316058\n",
            "2400\n",
            "1.21698509901762 -0.534005044028163\n",
            "25.01496265564018\n",
            "3000\n",
            "1.0101010091602802 -0.7605736143887043\n",
            "25.017482728582774\n",
            "done, took 723.966 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9a4d6bbfd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sZPsEmYKkxO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a8b7584-58f8-4af9-e322-e73bd5617a5e"
      },
      "source": [
        "# Test the agent\n",
        "hist = agent.test(\n",
        "    env_h,\n",
        "    nb_episodes=1,\n",
        "    action_repetition=5,\n",
        "    visualize=False,\n",
        "    nb_max_episode_steps=2000\n",
        ")"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing for 1 episodes ...\n",
            "600\n",
            "0.004661421553464606 0.004932353476760909\n",
            "1.9999999987900168\n",
            "1200\n",
            "0.003993377322331071 0.004229191836202517\n",
            "1.9999999985314876\n",
            "1800\n",
            "0.0038366397347999737 0.004064121458213776\n",
            "1.9999999983618775\n",
            "2400\n",
            "0.0037925616197753698 0.004017694300273433\n",
            "1.999999998237156\n",
            "3000\n",
            "0.0037890080420766026 0.0040139526390703395\n",
            "1.9999999981389447\n",
            "3600\n",
            "0.0037890116800554097 0.0040139526390703395\n",
            "1.9999999980594598\n",
            "4200\n",
            "0.0037890116800554097 0.0040139526390703395\n",
            "1.9999999979955643\n",
            "4800\n",
            "0.0037890116800554097 0.0040139526390703395\n",
            "1.9999999979442151\n",
            "5400\n",
            "0.0037890116800554097 0.0040139526390703395\n",
            "1.9999999979029326\n",
            "6000\n",
            "0.0037890116800554097 0.0040139526390703395\n",
            "1.9999999978697698\n",
            "6600\n",
            "0.0037890116800554097 0.0040139526390703395\n",
            "1.9999999978430978\n",
            "7200\n",
            "0.0037890116800554097 0.0040139526390703395\n",
            "1.9999999978216998\n",
            "7800\n",
            "0.0037890116800554097 0.0040139526390703395\n",
            "1.9999999978044514\n",
            "8400\n",
            "0.0037890116800554097 0.0040139526390703395\n",
            "1.999999997790649\n",
            "9000\n",
            "0.0037890116800554097 0.0040139526390703395\n",
            "1.9999999977795593\n",
            "9600\n",
            "0.0037890116800554097 0.0040139526390703395\n",
            "1.9999999977705638\n",
            "Episode 1: reward: -20000.000, steps: 2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eICS9QTKkxP"
      },
      "source": [
        ""
      ],
      "execution_count": 35,
      "outputs": []
    }
  ]
}