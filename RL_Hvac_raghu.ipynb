{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_Hvac_raghu.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RaghunandanVenkatesh/LearningToSee/blob/master/RL_Hvac_raghu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YfFFC-fHwjX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b188524-1848-479f-fdc5-1f7834c28d4d"
      },
      "source": [
        "!pip install keras-rl2"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras-rl2 in /usr/local/lib/python3.7/dist-packages (1.0.4)\n",
            "Requirement already satisfied: tensorflow>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from keras-rl2) (2.4.1)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (2.10.0)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.32.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (3.7.4.3)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.12.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (2.4.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (0.10.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.1.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (0.3.3)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (2.4.1)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.12)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (3.12.4)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (0.36.2)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.1.2)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.19.5)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (1.27.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (54.0.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (0.4.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (1.8.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (2.23.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (1.24.3)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (3.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kl6EXqpINmPA"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import UnivariateSpline\n",
        "\n",
        "import math\n",
        "import gym\n",
        "from gym import spaces, logger\n",
        "from gym.utils import seeding\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from rl.agents import DDPGAgent\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.random import OrnsteinUhlenbeckProcess\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Input, Concatenate\n",
        "from tensorflow.keras import initializers, regularizers\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkUpOfRnedCy"
      },
      "source": [
        "\n",
        "\n",
        "#inputs for plant\n",
        "T_oat = 30.0\n",
        "PWM_front_box = 30.0\n",
        "T_enginewater_set = 80.0\n",
        "POS_fresh_air_flap = 40.0\n",
        "value = 0\n",
        "dt = 1 # sample time"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlbAVjS_jHFt"
      },
      "source": [
        "class HvacPlantEnv(gym.Env):\n",
        "    def __init__(self, T_oat, T_enginewater_set, T_set):\n",
        "        self.dt = 1 # sample time\n",
        "        # initialization \n",
        "        water_val_pos_list = np.array([0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100])\n",
        "        A_list = np.array([0, 0.6011, .61, 0.6, 1.88, 1.88, 2.1, 2.1, 2.1, 2.5, 3.5])\n",
        "        self.lookup = UnivariateSpline(water_val_pos_list, A_list, k=1, s=0.0)\n",
        "        self.hA_screen = 0.0007\n",
        "        self.hA_shell = 0.0029\n",
        "        self.convfactor = 0.0016\n",
        "        self.mcp_shell = 0.83\n",
        "        self.T_oat = T_oat\n",
        "        self.T_enginewater_set = T_enginewater_set\n",
        "        self.T_set = T_set\n",
        "        self.action_space = spaces.Tuple((\n",
        "                                spaces.Discrete(100),\n",
        "                                spaces.Discrete(100)))\n",
        "        self.observation_space = spaces.Box(-40, 80, shape=(4,) ,dtype=np.float32)\n",
        "\n",
        "\n",
        "        self.state = None\n",
        "\n",
        "        self.steps_beyond_done = None\n",
        "\n",
        "    def step(self, action):\n",
        "        T_shell, T_fap, T_enginewater, value = self.state\n",
        "        POS_fresh_air_flap = action[0]\n",
        "        PWM_front_box = action[1] \n",
        "        # engine water temp\n",
        "        T_enginewater = self.T_oat + ( self.T_enginewater_set + self.T_oat - T_enginewater) * (1 - np.exp(-value)) \n",
        "        value = np.min([value+0.002, 5])\n",
        "        # air outlet temp\n",
        "        T_air_in = self.T_oat * POS_fresh_air_flap/100 + T_fap * (1 - POS_fresh_air_flap/100)\n",
        "        A = self.lookup(POS_fresh_air_flap)\n",
        "        eff = 1 - np.exp(-POS_fresh_air_flap*A/100)\n",
        "        T_airout = T_air_in + (T_enginewater - T_air_in) * eff \n",
        "        # room temperature\n",
        "        d_T_fap = self.hA_screen * (self.T_oat - T_fap) + self.hA_shell * (T_shell - T_fap)\n",
        "        d_T_fap += PWM_front_box * self.convfactor * 0.718 * (T_airout - T_fap)\n",
        "        T_fap = d_T_fap * self.dt + T_fap\n",
        "        T_shell = T_shell - self.hA_shell * (T_shell - T_fap)/self.mcp_shell \n",
        "        self.state = (T_shell, T_fap, T_enginewater, value)   \n",
        "\n",
        "        #reward\n",
        "        done  = self.T_set == T_fap\n",
        "        if not done:\n",
        "            reward = np.abs(self.T_set - T_fap)\n",
        "        elif self.steps_beyond_done is  None:\n",
        "            self.steps_beyond_done = 0\n",
        "            reward = np.abs(self.T_set - T_fap)\n",
        "        else:\n",
        "            if self.steps_beyond_done == 0:\n",
        "                print(\n",
        "                  \"You are calling 'step()' even though this \"\n",
        "                  \"environment has already returned done = True. You \"\n",
        "                  \"should always call 'reset()' once you receive 'done = \"\n",
        "                  \"True' -- any further steps are undefined behavior.\")\n",
        "            self.steps_beyond_done += 1\n",
        "            reward = 0.0\n",
        "        return np.array(self.state), reward, done, {} \n",
        "\n",
        "    def reset(self):\n",
        "        # self.T_oat = T_oat\n",
        "        # self.T_enginewater_set = T_enginewater\n",
        "        self.state = [self.T_oat, self.T_oat, self.T_oat, 0]\n",
        "        self.steps_beyond_done = None\n",
        "        return np.array(self.state)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGCVSFnBu2Tz",
        "outputId": "aaa27300-0c10-4d53-bd8b-3d595cb9dbbf"
      },
      "source": [
        "action = spaces.Tuple((spaces.Discrete(10), spaces.Discrete(100))).sample()\n",
        "env_h = HvacPlantEnv(24,80,30)\n",
        "env_h.reset()\n",
        "env_h.step(action)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([2.4e+01, 2.4e+01, 2.4e+01, 2.0e-03]), 6.0, False, {})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdOL2AG9Kkw3"
      },
      "source": [
        "# env_h.action_space.sample()"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXoVnjwlI3LC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cb61ae9-2040-446f-9b6c-582d434e0c9e"
      },
      "source": [
        "state = env_h.reset()\n",
        "action = env_h.action_space.sample()\n",
        "obs, rew , done, _ = env_h.step(action)\n",
        "print(state.shape)\n",
        "print(env_h.observation_space.shape)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4,)\n",
            "(4,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aG2uT4Q5Kkw8"
      },
      "source": [
        "# action_list = []\n",
        "# for _ in range(50):\n",
        "#     action = env_h.action_space.sample()\n",
        "#     # print(action)\n",
        "#     action_list.append(action)\n",
        "# for i in range(len(action_list)):\n",
        "#     obs,rew,done,_ = env_h.step(action_list[i])\n",
        "#     print(obs)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuchMdz6Kkw-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe98280e-3fe1-4a93-ff9a-3129e69df61f"
      },
      "source": [
        "nb_steps =100\n",
        "cum_rew = 0\n",
        "i = 0\n",
        "for i in tqdm(range(nb_steps)): \n",
        "    action = env_h.action_space.sample()\n",
        "    x, reward, done, _ = env_h.step(action)\n",
        "    #print(reward)\n",
        "    cum_rew += rew\n",
        "    i+=1\n",
        "print(cum_rew/i)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 7196.44it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "6.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pctMfvheKkxA"
      },
      "source": [
        "DDPG Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2zmRYLNKkxC"
      },
      "source": [
        "nb_actions = len(env_h.action_space.sample())"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "94CVJBTGKkxD"
      },
      "source": [
        "window_length = 1"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rlsD-ckKkxE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25a4e7b3-0c55-4d0d-b2fc-e591e8ce1945"
      },
      "source": [
        "actor = Sequential()\n",
        "# The network's input fits the observation space of the env\n",
        "actor.add(Flatten(input_shape=(window_length,) + (4,)))  # observation_space.shape != (4,).. ## todo: correct it\n",
        "actor.add(Dense(16, activation='relu'))\n",
        "actor.add(Dense(17, activation='relu'))\n",
        "# The network output fits the action space of the env\n",
        "actor.add(Dense(nb_actions,\n",
        "                kernel_initializer=initializers.RandomNormal(stddev=1e-5),\n",
        "                activation='sigmoid',\n",
        "                kernel_regularizer=regularizers.l2(1e-2)))\n",
        "actor.add(tf.keras.layers.Lambda(lambda x: x * 100))\n",
        "print(actor.summary())"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_3 (Flatten)          (None, 4)                 0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 16)                80        \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 17)                289       \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 2)                 36        \n",
            "_________________________________________________________________\n",
            "lambda_1 (Lambda)            (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 405\n",
            "Trainable params: 405\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpSJHU7eKkxH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ee3278f-7c6e-4f27-c821-9f7f01ac2301"
      },
      "source": [
        "#todo : Normalise inputs(action and oservation)\n",
        "action_input = Input(shape=(nb_actions,), name='action_input')\n",
        "observation_input = Input(shape=(window_length,) + (4,), name='observation_input')\n",
        "# (using keras functional API)\n",
        "flattened_observation = Flatten()(observation_input)\n",
        "x = Concatenate()([action_input, flattened_observation])\n",
        "x = Dense(32, activation='relu')(x)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "#x = Dense(32, activation='relu')(x)\n",
        "x = Dense(1, activation='linear')(x)\n",
        "critic = Model(inputs=(action_input, observation_input), outputs=x)\n",
        "print(critic.summary())"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "observation_input (InputLayer)  [(None, 1, 4)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "action_input (InputLayer)       [(None, 2)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_4 (Flatten)             (None, 4)            0           observation_input[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 6)            0           action_input[0][0]               \n",
            "                                                                 flatten_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 32)           224         concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 32)           1056        dense_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 1)            33          dense_12[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 1,313\n",
            "Trainable params: 1,313\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYFbiexhKkxJ"
      },
      "source": [
        "memory = SequentialMemory(\n",
        "    limit=5000,\n",
        "    window_length=1\n",
        ")"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1syBDsreKkxK"
      },
      "source": [
        "# Create a random process for exploration during training\n",
        "# this is essential for the DDPG algorithm\n",
        "random_process = OrnsteinUhlenbeckProcess(\n",
        "    theta=0.5,\n",
        "    mu=0.0,\n",
        "    sigma=0.1,\n",
        "    dt=0.001,\n",
        "    sigma_min=0.05,\n",
        "    n_steps_annealing=85000,\n",
        "    size=2\n",
        ")"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBj_qQFdKkxL"
      },
      "source": [
        "agent = DDPGAgent(\n",
        "    # Pass the previously defined characteristics\n",
        "    nb_actions=nb_actions,\n",
        "    actor=actor,\n",
        "    critic=critic,\n",
        "    critic_action_input=action_input,\n",
        "    memory=memory,\n",
        "    random_process=random_process,\n",
        "\n",
        "    # Define the overall training parameters\n",
        "    nb_steps_warmup_actor=2048,\n",
        "    nb_steps_warmup_critic=1024,\n",
        "    target_model_update=1000,\n",
        "    gamma=0.99,\n",
        "    batch_size=128,\n",
        "    memory_interval=2\n",
        ")"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bcStwpZKkxM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "2cc8fb10-f06a-42a5-d00b-f99fb250d3a8"
      },
      "source": [
        "agent.compile([Adam(lr=3e-5), Adam(lr=3e-3)])\n",
        "#agent.compile(Adam(lr=3e-5))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-9416008b7ac1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#agent.compile(Adam(lr=3e-5))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rl/agents/ddpg.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, metrics)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmean_q\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute '_name'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6zOi9yrKkxM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afcaeb8a-dcaa-457d-f22e-f81e8d90deb4"
      },
      "source": [
        "# Training\n",
        "agent.fit(\n",
        "    env_h,\n",
        "    nb_steps=10000,\n",
        "    nb_max_start_steps=0,\n",
        "    nb_max_episode_steps=500,\n",
        "    visualize=False,\n",
        "    action_repetition=1,\n",
        "    verbose=2,\n",
        "    log_interval=50,\n",
        "\n",
        ")"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 10000 steps ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  500/10000: episode: 1, duration: 0.534s, episode steps: 500, steps per second: 937, episode reward: 4472.278, mean reward:  8.945 [ 0.004, 16.218], mean action: 50.010 [49.962, 50.077],  loss: --, mean_q: --\n",
            " 1000/10000: episode: 2, duration: 0.397s, episode steps: 500, steps per second: 1259, episode reward: 4465.884, mean reward:  8.932 [ 0.005, 16.200], mean action: 49.880 [49.757, 49.988],  loss: --, mean_q: --\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " 1500/10000: episode: 3, duration: 4.079s, episode steps: 500, steps per second: 123, episode reward: 4470.225, mean reward:  8.940 [ 0.006, 16.216], mean action: 50.000 [49.890, 50.088],  loss: 18.649503, mean_q: 1.347735\n",
            " 2000/10000: episode: 4, duration: 3.643s, episode steps: 500, steps per second: 137, episode reward: 4467.129, mean reward:  8.934 [ 0.005, 16.205], mean action: 49.898 [49.802, 49.972],  loss: 5.729023, mean_q: 5.187666\n",
            " 2500/10000: episode: 5, duration: 4.012s, episode steps: 500, steps per second: 125, episode reward: 3515.586, mean reward:  7.031 [ 0.009, 10.414], mean action: 40.779 [2.193, 78.139],  loss: 12.272180, mean_q: 12.618018\n",
            " 3000/10000: episode: 6, duration: 4.019s, episode steps: 500, steps per second: 124, episode reward: 874.646, mean reward:  1.749 [ 0.001,  6.000], mean action: 25.299 [0.867, 66.762],  loss: 6.985740, mean_q: 13.822590\n",
            " 3500/10000: episode: 7, duration: 3.987s, episode steps: 500, steps per second: 125, episode reward: 1902.809, mean reward:  3.806 [ 2.618,  6.000], mean action: 27.752 [0.353, 56.624],  loss: 8.244351, mean_q: 19.640537\n",
            " 4000/10000: episode: 8, duration: 4.034s, episode steps: 500, steps per second: 124, episode reward: 2279.051, mean reward:  4.558 [ 3.587,  6.000], mean action: 28.570 [0.300, 59.038],  loss: 4.102958, mean_q: 19.991024\n",
            " 4500/10000: episode: 9, duration: 4.061s, episode steps: 500, steps per second: 123, episode reward: 2638.348, mean reward:  5.277 [ 4.838,  6.000], mean action: 45.635 [0.067, 99.384],  loss: 4.636744, mean_q: 24.350172\n",
            " 5000/10000: episode: 10, duration: 4.087s, episode steps: 500, steps per second: 122, episode reward: 2857.277, mean reward:  5.715 [ 5.505,  6.000], mean action: 49.612 [0.022, 99.739],  loss: 3.025073, mean_q: 24.353746\n",
            " 5500/10000: episode: 11, duration: 3.992s, episode steps: 500, steps per second: 125, episode reward: 3052.096, mean reward:  6.104 [ 5.938,  6.452], mean action: 49.773 [-0.111, 99.867],  loss: 3.440891, mean_q: 29.870569\n",
            " 6000/10000: episode: 12, duration: 4.039s, episode steps: 500, steps per second: 124, episode reward: 2946.965, mean reward:  5.894 [ 5.773,  6.000], mean action: 49.894 [-0.021, 99.924],  loss: 2.005681, mean_q: 29.921152\n",
            " 6500/10000: episode: 13, duration: 3.998s, episode steps: 500, steps per second: 125, episode reward: 2702.558, mean reward:  5.405 [ 4.694,  6.000], mean action: 50.049 [0.150, 99.986],  loss: 3.088705, mean_q: 35.855198\n",
            " 7000/10000: episode: 14, duration: 4.034s, episode steps: 500, steps per second: 124, episode reward: 2770.718, mean reward:  5.541 [ 4.970,  6.000], mean action: 49.951 [0.103, 99.891],  loss: 1.935837, mean_q: 35.892574\n",
            " 7500/10000: episode: 15, duration: 4.046s, episode steps: 500, steps per second: 124, episode reward: 2966.542, mean reward:  5.933 [ 5.892,  6.000], mean action: 49.946 [-0.021, 99.939],  loss: 3.028252, mean_q: 41.772301\n",
            " 8000/10000: episode: 16, duration: 4.105s, episode steps: 500, steps per second: 122, episode reward: 3025.011, mean reward:  6.050 [ 5.983,  6.112], mean action: 49.960 [-0.050, 99.969],  loss: 1.921106, mean_q: 41.826618\n",
            " 8500/10000: episode: 17, duration: 4.112s, episode steps: 500, steps per second: 122, episode reward: 2757.713, mean reward:  5.515 [ 4.882,  6.000], mean action: 50.132 [0.100, 100.142],  loss: 2.956284, mean_q: 47.622002\n",
            " 9000/10000: episode: 18, duration: 4.144s, episode steps: 500, steps per second: 121, episode reward: 3119.975, mean reward:  6.240 [ 6.000,  6.409], mean action: 49.943 [-0.124, 100.051],  loss: 1.902788, mean_q: 47.642387\n",
            " 9500/10000: episode: 19, duration: 4.190s, episode steps: 500, steps per second: 119, episode reward: 3128.681, mean reward:  6.257 [ 6.000,  6.540], mean action: 49.938 [-0.109, 99.984],  loss: 2.737880, mean_q: 53.265804\n",
            " 10000/10000: episode: 20, duration: 4.103s, episode steps: 500, steps per second: 122, episode reward: 3188.962, mean reward:  6.378 [ 6.000,  6.997], mean action: 49.931 [-0.167, 100.036],  loss: 1.846625, mean_q: 53.236423\n",
            "done, took 73.661 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f2fd2c67610>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sZPsEmYKkxO"
      },
      "source": [
        "# Test the agent\n",
        "hist = agent.test(\n",
        "    env_h,\n",
        "    nb_episodes=1,\n",
        "    action_repetition=5,\n",
        "    visualize=False,\n",
        "    nb_max_episode_steps=2000\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eICS9QTKkxP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}